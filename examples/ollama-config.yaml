# Example Configuration for Ollama
# Copy this to llm-config.yaml in the root directory

provider: ollama

llm:
  model_name: qwen3-coder
  base_url: http://localhost:11434
  temperature: 0.1
  max_tokens: 4096
  timeout: 60

rag:
  chunk_size: 1000
  chunk_overlap: 200
  max_results: 5
  embedding_model: sentence-transformers/all-MiniLM-L6-v2

maf:
  enabled: true
  max_iterations: 10
  timeout: 300

auto_trigger:
  enabled: true
  threshold: 0.7
  show_activity: true
