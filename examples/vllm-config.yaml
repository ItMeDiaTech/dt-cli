# Example Configuration for vLLM (Production)
# Copy this to llm-config.yaml in the root directory

provider: vllm

llm:
  model_name: qwen3-coder
  base_url: http://localhost:8000
  api_key: not-needed
  temperature: 0.1
  max_tokens: 4096
  timeout: 120

rag:
  chunk_size: 1000
  chunk_overlap: 200
  max_results: 5
  embedding_model: BAAI/bge-base-en-v1.5  # Better for code

maf:
  enabled: true
  max_iterations: 10
  timeout: 300

auto_trigger:
  enabled: true
  threshold: 0.7
  show_activity: true

# To start vLLM server:
# vllm serve qwen3-coder --port 8000
#
# For multi-GPU:
# vllm serve qwen3-coder --tensor-parallel-size 4 --port 8000
