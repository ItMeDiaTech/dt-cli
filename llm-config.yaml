# dt-cli LLM Configuration (Updated for 2025)
#
# This file configures which LLM provider to use.
# dt-cli supports multiple open source and proprietary providers.
#
# Based on latest benchmarks: HumanEval, LiveCodeBench, BigCodeBench (Jan 2025)
#
# Recommended for development: ollama
# Recommended for production: vllm
# Optional: claude (requires Anthropic subscription)

# ============================================================================
# OPTION 1: Ollama (Recommended for Development) - 100% OPEN SOURCE
# ============================================================================
#
# Ollama is perfect for local development:
# - Free and open source
# - Runs on your machine
# - Easy setup: https://ollama.com/download
# - Best models: qwen2.5-coder, deepseek-r1-distill-qwen, deepseek-v3
#
provider: ollama

llm:
  # RECOMMENDED: Qwen 2.5-Coder is #1 open-source coding LLM (May 2025)
  # HumanEval: 75-80% | LiveCodeBench: 38.7% | Best for: code generation
  model_name: qwen2.5-coder:32b

  # ALTERNATIVES (uncomment to use):
  # model_name: qwen2.5-coder:7b                 # For 8-16GB RAM systems
  # model_name: qwen2.5-coder:72b                # For maximum performance (48+ GB RAM)
  # model_name: deepseek-r1-distill-qwen-32b     # NEW! Best reasoning (outperforms o1-mini)
  # model_name: deepseek-v3                      # Strong alternative, better at math/reasoning
  # model_name: codestral:25.01                  # Mid-size option for resource constraints

  # OUTDATED (not recommended for new deployments):
  # model_name: codellama:34b    # Superseded by Qwen (53.7% vs 75-80% HumanEval)
  # model_name: starcoder2       # Significantly behind current leaders (~50% HumanEval)

  base_url: http://localhost:11434
  temperature: 0.1
  max_tokens: 4096
  timeout: 60

# ============================================================================
# OPTION 2: vLLM (Recommended for Production) - 100% OPEN SOURCE
# ============================================================================
#
# vLLM is perfect for production deployment:
# - 19.3x higher throughput than Ollama (793 TPS vs 41 TPS)
# - 8.4x lower P99 latency (80ms vs 673ms)
# - Production-grade performance with PagedAttention
# - Supports multi-GPU deployment
# - 100% open source
#
# To use vLLM, uncomment the section below and comment out Ollama above:

# provider: vllm
#
# llm:
#   # RECOMMENDED for production: Qwen 2.5-Coder or DeepSeek-R1
#   model_name: qwen2.5-coder:32b
#   # model_name: deepseek-r1-distill-qwen-32b  # Best reasoning
#
#   base_url: http://localhost:8000  # vLLM server endpoint
#   api_key: not-needed  # vLLM doesn't require authentication by default
#   temperature: 0.1
#   max_tokens: 4096
#   timeout: 120
#
#   # Production optimization (optional but recommended)
#   vllm_config:
#     tensor_parallel_size: 2          # Multi-GPU support (if available)
#     max_model_len: 16384             # Extended context window
#     gpu_memory_utilization: 0.95     # Maximize GPU usage
#     dtype: bfloat16                  # Optimal precision/performance
#     max_num_seqs: 256                # Concurrent sequences
#     enforce_eager: false             # Use CUDA graphs for speed
#
# Expected performance: +50-100% throughput, -30-50% latency vs default settings

# ============================================================================
# OPTION 3: DeepSeek-R1 Distilled (NEW - Jan 2025) - 100% OPEN SOURCE
# ============================================================================
#
# DeepSeek-R1 distilled models are the latest breakthrough:
# - Outperforms OpenAI o1-mini on many benchmarks
# - HumanEval: 80%+ (vs 75-80% for Qwen 2.5-Coder)
# - Exceptional reasoning capabilities
# - Best for: Complex problem solving, multi-step reasoning, correctness
#
# To try this NEW model, uncomment below:

# provider: ollama
#
# llm:
#   model_name: deepseek-r1-distill-qwen-32b  # Best reasoning + coding combo
#   base_url: http://localhost:11434
#   temperature: 0.1
#   max_tokens: 4096
#   timeout: 60

# ============================================================================
# OPTION 4: Claude Code (Optional) - PROPRIETARY
# ============================================================================
#
# Claude is OPTIONAL and only for users who:
# - Already have an Anthropic subscription
# - Prefer Claude for certain tasks
# - Want a hybrid setup
#
# NOTE: dt-cli works best with 100% open source providers.
#
# To use Claude, uncomment the section below and add your API key:

# provider: claude
#
# llm:
#   model_name: claude-sonnet-4.5
#   api_key: <your-anthropic-api-key>  # Get from https://console.anthropic.com/
#   temperature: 0.1
#   max_tokens: 4096
#   timeout: 60

# ============================================================================
# RAG Configuration (Optimized for Code - Same for All Providers)
# ============================================================================

rag:
  chunk_size: 1000
  chunk_overlap: 200
  max_results: 5

  # CODE-OPTIMIZED EMBEDDINGS (+15-20% better than all-MiniLM)
  embedding_model: BAAI/bge-base-en-v1.5  # Best for code embeddings
  # embedding_model: sentence-transformers/all-MiniLM-L6-v2  # Fallback: smaller, faster

  # AST-BASED CHUNKING (+25-40% better than naive text splitting)
  use_ast_chunking: true  # Use tree-sitter AST chunking
  use_instruction_prefix: true  # Use instruction prefix for BGE models

# ============================================================================
# MAF (Multi-Agent Framework) Configuration
# ============================================================================

maf:
  enabled: true
  max_iterations: 10
  timeout: 300

# ============================================================================
# Auto-Trigger Configuration (Intelligent Query Routing)
# ============================================================================
#
# Intelligent auto-triggering that automatically:
# - Classifies query intent (code_search, debugging, review, etc.)
# - Decides when to use RAG vs direct LLM
# - Routes to appropriate agents (debug, review, graph)
# - Tracks conversation context
#
# Expected impact: 70% reduction in manual /rag-query commands!

auto_trigger:
  enabled: true  # Enable intelligent auto-triggering
  threshold: 0.7  # Confidence threshold for intent classification (0.0-1.0)
  show_activity: true  # Show activity indicators (e.g., "üîç Searching codebase...")
  max_context_tokens: 8000  # Maximum tokens to use for context

  # Intent classification model
  intent_model: all-MiniLM-L6-v2  # Lightweight model for fast classification

  # Auto-trigger rules (when to automatically use RAG)
  rules:
    code_search: true  # Auto-trigger RAG for code search queries
    graph_query: true  # Auto-trigger knowledge graph for dependency queries
    debugging: true  # Auto-trigger debug agent + RAG
    code_review: true  # Auto-trigger review agent + RAG
    documentation: true  # Auto-trigger RAG for docs queries
    direct_answer: false  # Skip RAG for simple edits (typos, comments, etc.)

# ============================================================================
# Quick Setup Guide
# ============================================================================
#
# RECOMMENDED SETUP (Development):
#   1. Install Ollama: https://ollama.com/download
#   2. Pull model: ollama pull qwen2.5-coder:32b
#   3. Start Ollama: ollama serve
#   4. Use config above (already configured!)
#
# TRY THE NEW REASONING MODEL:
#   1. Pull model: ollama pull deepseek-r1-distill-qwen-32b
#   2. Update model_name above to: deepseek-r1-distill-qwen-32b
#   3. Restart server
#
# FOR RESOURCE-CONSTRAINED SYSTEMS (8-16GB RAM):
#   1. Use smaller model: ollama pull qwen2.5-coder:7b
#   2. Update model_name above to: qwen2.5-coder:7b
#   3. Reduce max_tokens to 2048 if needed
#
# PRODUCTION SETUP (vLLM):
#   1. Install vLLM: pip install vllm
#   2. Start server: vllm serve qwen2.5-coder:32b --port 8000 --tensor-parallel-size 2
#   3. Enable OPTION 2 above (vLLM Production)
#   4. Restart dt-cli server
#
# FOR CLAUDE (Optional):
#   1. Get API key from https://console.anthropic.com/
#   2. Update model_name and api_key in OPTION 4 above
#   3. Change provider to 'claude'

# ============================================================================
# Model Selection Decision Guide
# ============================================================================
#
# Choose your model based on your use case:
#
# Development (Local Machine):
#   - 8-16 GB RAM? ‚Üí qwen2.5-coder:7b
#   - 32+ GB RAM? ‚Üí qwen2.5-coder:32b (RECOMMENDED)
#   - Need best reasoning? ‚Üí deepseek-r1-distill-qwen-32b
#
# Production (Server):
#   - Single GPU (24-48 GB)? ‚Üí vLLM + deepseek-r1-distill-qwen-32b
#   - Multi-GPU (80+ GB)? ‚Üí vLLM + qwen2.5-coder:72b
#   - Resource-constrained? ‚Üí vLLM + codestral:25.01
#
# Edge/Embedded:
#   - 4-8 GB RAM? ‚Üí qwen2.5-coder:1.5b or codestral:7b
#
# ============================================================================
# Performance Benchmarks (Reference)
# ============================================================================
#
# HumanEval Pass@1 (164 coding problems, functional correctness):
#   - Qwen 2.5-Coder-32B:           75-80% ‚≠ê #1 Open Source
#   - DeepSeek-R1-Distill-Qwen-32B: 80%+   ‚≠ê Best Reasoning
#   - DeepSeek-V3:                  72-75% ‚≠ê Strong Alternative
#   - Codestral 25.01:              ~65%   ‚≠ê Mid-size Option
#   - CodeLlama 34B:                53.7%  ‚ö†Ô∏è Outdated
#   - StarCoder2:                   ~50%   ‚ö†Ô∏è Outdated
#
# LiveCodeBench (Dynamic, contamination-free):
#   - Qwen 2.5 Max:                 38.7%
#   - DeepSeek-V3:                  37.6%
#   - DeepSeek-R1:                  ~40%+
#
# vLLM vs Ollama Performance:
#   - Throughput:  vLLM 793 TPS vs Ollama 41 TPS (19.3x faster)
#   - Latency P99: vLLM 80ms vs Ollama 673ms (8.4x better)
#   - Concurrent:  vLLM scales linearly, Ollama caps at 22 RPS
#
# Last Updated: January 2025
# See docs/LLM_RECOMMENDATIONS_2025.md for detailed analysis
